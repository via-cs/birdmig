\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{titlesec}
\usepackage{authblk}
\usepackage[colorlinks=true, urlcolor=blue, linkcolor=black]{hyperref}
\usepackage{fancyvrb}

\title{Visual Analytics of the Impacts of Climate Change on 
Migratory Bird Habitats \\
 \Large{Technical Document}}
\author{Jacob Vogt, Mihika Krishna, Catherine Kang, Hangyul Yun}
\affil{Professor Dongyu Liu}
\affil{University of California, Davis}
\date{June 2024}

\begin{document}

\maketitle

\tableofcontents

\newpage

\section*{Introduction}
Our Senior Capstone project is comprised of two components:

\begin{enumerate}
  \item A species distribution model (SDM) capable of predicting how climate change will affect future bird habitats up to year 2100
  \item A web-app that visualizes SDM output and displays other relevant information such as bird migration patterns and climate trends.
\end{enumerate}

This technical document will overview how each of these components work, as well as the required data and file structure for them to operate correctly.

\section{Species Distribution Model}

\subsection{Datasets}

The species distribution model (SDM) uses two datasets, one for climate data and one for species distribution data. 

\subsubsection*{Climate Data}

NEX-GDDP-CMIP6 datasets are granular by year and offered for four (future) Shared Socioeconomic Pathways (SSP). These dastasets are offered in NETCDF (.nc) format. 
Descriptions of each SSP can be found \href{https://www.carbonbrief.org/explainer-how-shared-socioeconomic-pathways-explore-future-climate-change/}{at this link}.
\\\\
Climate scenarios used were from the NEX-GDDP-CMIP6 dataset, prepared by the
Climate Analytics Group and NASA Ames Research Center using the NASA Earth
Exchange and distributed by the NASA Center for Climate Simulation (NCCS).
\\\\
\textit{
Thrasher, B., Wang, W., Michaelis, A. et al. NASA Global Daily Downscaled
Projections, CMIP6. Sci Data 9, 262 (2022). \url{https://doi.org/10.1038/s41597-022-01393-4}
\\\\
Thrasher, B., Wang, W., Michaelis, A. Nemani, R. (2021). NEX-GDDP-CMIP6. NASA
Center for Climate Simulation. \url{https://doi.org/10.7917/OFSG3345}}

\subsubsection*{Species Distribution Data}

The occurrence datasets for our bird species come from the Global Biodiversity Information Facility (GBIF). The citations for each dataset is below.
\textit{
\begin{itemize}
    \item Numenius americanus Bechstein, 1812 in GBIF Secretariat (2023). GBIF Backbone Taxonomy. Checklist dataset \url{https://doi.org/10.15468/39omei} accessed via GBIF.org on 2024-06-09.
    \item Setophaga striata (J.R. Forster, 1772) in GBIF Secretariat (2023). GBIF Backbone Taxonomy. Checklist dataset \url{https://doi.org/10.15468/39omei} accessed via GBIF.org on 2024-06-09.
    \item Anser albifrons (Scopoli, 1769) in GBIF Secretariat (2023). GBIF Backbone Taxonomy. Checklist dataset \url{https://doi.org/10.15468/39omei} accessed via GBIF.org on 2024-06-09.
    \item Haliaeetus leucocephalus (Linnaeus, 1766) in GBIF Secretariat (2023). GBIF Backbone Taxonomy. Checklist dataset \url{https://doi.org/10.15468/39omei} accessed via GBIF.org on 2024-06-09.
    \item Numenius phaeopus subsp. phaeopus in GBIF Secretariat (2023). GBIF Backbone Taxonomy. Checklist dataset \url{https://doi.org/10.15468/39omei} accessed via GBIF.org on 2024-06-09.
\end{itemize}
}
GBIF datasets are downloaded in .csv format.

\subsection{R Preprocessing}

Pre-processing is necessary to transform raw datasets into a form factor that is usable by our SDM.
This pre-processing step uses R due to its specialty in working with biological and spatial data.
\\\\
There are two different R scripts, one for preprocessing training data and another for preprocessing prediction data. These scripts are named \\
\texttt{\href{run:../model/preprocess\_training.R}{preprocess\_training.R}} and \texttt{\href{run:../model/preprocess\_prediction.R}{preprocess\_prediction.R}} respectively.

\subsubsection*{Packages}

\begin{itemize}
	\item \textit{dismo}: Methods for species distribution modeling, that is, predicting the environmental similarity of any site to that of the locations of known occurrences of a species.
	\item \textit{sf}: Support for simple features, a standardized way to encode spatial vector data. Binds to 'GDAL' for reading and writing data, to 'GEOS' for geometrical operations, and to 'PROJ' for projection conversions and datum transformations. Uses by default the 's2' package for spherical geometry operations on ellipsoidal (long/lat) coordinates.
	\item \textit{raster}: Reading, writing, manipulating, analyzing and modeling of spatial data. 
	\item \textit{ncdf4}: Provides a high-level R interface to data files written using Unidata's netCDF library (version 4 or earlier), which are binary data files that are portable across platforms and include metadata information in addition to the data sets.
\end{itemize}

\subsubsection*{Prerequisites}

The R scripts were developed using the latest version of R as of June 9, 2024 (v4.4.0). Different versions of R including v4.4.0 can be found at \url{https://cran.r-project.org/bin/windows/base/old/}. Versions older than v4.3.2 are not guaranteed to work.
\\\\
RStudio is recommended to run the R scripts. Running the scripts from an independent R terminal could work in theory, but is not tested. RStudio can be installed from \url{https://posit.co/download/rstudio-desktop/}
\\\\
The above packages should either be installed with RStudio's built-in package installer or by running the command \texttt{install.packages(c("sf", "raster", "dismo", "ncdf4"))} in an R terminal.
\\\\
Both of these scripts require datasets that can be downloaded from the Box folder at \url{https://ucdavis.box.com/s/l5iky1y6z526r6ewifvtr6c4ccp5jsjz}. The \texttt{data} folder from the above link should be placed in the root folder of the repository. 

\subsubsection*{Running the Scripts}

If the above prerequisites are met, then the R scripts should run without issue. To do this in RStudio, open the desired script and click the "Source" button.
\\\\
If a custom species is desired, the species CSV must be downloaded from GBIF and stored under \texttt{/data/GBIF/[species name].csv}. Furthermore, the \texttt{species\_list} parameter in \texttt{training-preprocessing.R} must be altered to reflect the new species. 

\subsubsection*{Script Descriptions}
See the above section for instructions on obtaining the appropriate inputs.
\\\\
\texttt{\underline{{training-preprocessing.R}}}
\begin{verbatim}
	Input:	
			* Species datasets (.csv)
			* Historical climate datasets (.nc)
	Parameters:
			* species_list: an array of strings describing the desired species 
			* e: extent variable that describes longitude/latitude boundaries 
	Output:
			* Presence/absence files (.geojson)
			* Historical climate raster files (.tif)
\end{verbatim}

Presence/absence files are created by using 1) occurrences from the species CSV file as presences and 2) an equal number of climate points from the climate NetCDF file as pseudo-absences. Pseudo-absences are used due to lack of real absence data. GeoJSON files are stored under \texttt{/model/input/geojson}.

Climate raster files are converted from NetCDF to TIFF format for compatibility with python libraries. TIFF files are stored under \texttt{/model/input/historical}.

All outputs are cropped to the extent variable \texttt{e}. Default \texttt{e} is set to both Americas.
\\\\
\texttt{\underline{prediction-preprocessing.R}}
\begin{verbatim}
	Input:	
			* Future climate datasets (.nc)
	Parameters:
			* e: extent variable that describes longitude/latitude boundaries 
	Output:
			* Future climate raster files (.tif)
\end{verbatim}

Climate raster files are converted from NetCDF to TIFF format for compatibility with python libraries.
Unlike the climate rasters for model training (of which there are only two), there exists a pair of future rasters for each year for each SSP model, making a total of 680 raster files. 
TIFF files are stored under \texttt{/model/input/future}, organized by SSP model and year.
Raster files are cropped to the extent variable \texttt{e}. Default \texttt{e} is set to both Americas.

\subsection{Species Distribution Modeling in Python}

The SDM was prototyped in Jupyter Notebook (see \texttt{\href{run:../model/birdmig\_sdm.ipynb}{birdmig\_sdm.ipynb}}) and implemented in python script (see \texttt{\href{run:../model/model\_training.py}{model\_training.py}} and \texttt{\href{run:../model/model\_prediction.py}{model\_prediciton.py}}). Python scripts were used over Jupyter Notebook for implementation due to ease of use with loops and terminal.

\subsubsection*{Requirements}
Python scripts were developed using Python 3.10.12, which can be found \href{https://www.python.org/downloads/}{at this link}. While you may use newer versions of Python, they are not guaranteed to work.
\\\\
Required libraries include \texttt{os, tqdm, glob, pyimpute, pickle, rasterio, matplotlib, geopandas, sklearn, numpy,} and \texttt{pandas}. These libraries can be installed in a Python terminal using \texttt{pip}.
\\\\
Ensure that preprocessed data from R exists under \texttt{/model/input/}. There should be three folders: \texttt{geojson, historical,} and \texttt{future}, each containing the appropriate data. \texttt{geojson} and \texttt{historical} are required for model training, while \texttt{future} and \texttt{/data/pickle} is required for model predicting. Example pickle files can be found at the Box link \href{https://ucdavis.box.com/s/l5iky1y6z526r6ewifvtr6c4ccp5jsjz}{here}.

\subsubsection{SDM Training}
The model is trained by associating presence/absence data with zonal averages for temperature and precipitation. In other words, the model learns what zonal environmental factors typically correlate to a presence or absence of a species. When given a file containing possible future climate readings, the model can then predict how the distribution of a species may react to that climate.
\\\\
Training models for each bird species goes as follows:
\begin{enumerate}
	\item Data is loaded into training vectors using the \texttt{load\_data()} function
	\item The random forest classifier is chosen for model training
	\item Classifier's accuracy is measured by averaging K-fold evaluation (k=5)
	\item Model is fit with training vectors and saved to a pickle file
\end{enumerate}

\noindent Random forest was chosen for the model because it showed the highest and most consistent accuracy when compared to other classifiers (extra trees, SVM, xgboost, lightgbm) See \texttt{\href{run:../model/birdmig\_sdm.ipynb}{birdmig\_sdm.ipynb}}.
\\\\
A description of the \texttt{load\_data()} function is as follows:
\begin{Verbatim}[tabsize=4]
load_data():
	Parameters:
		species: string representing species name
		n: optional int representing sample size (default=None)
	Return:
		train_xs: 2D array of floats where each inner array represents 
				  zonal averages for temperature and precipitation 
		train_y:  array whose values represent presence or absence & 
				  are indexed to correlate to each array in train_xs
				  
	load_data() loads appropriate files from /model/input/ into training 
	vectors for SDM training. It takes an optional n parameter that
	randomly samples the presence/absence data in order to reduce the
	size of the training vector.
	
	load_data() primarily wraps the load_training_vectors() function 
	from pyimpute, which is responsible for finding the zonal averages
	to begin with. load_data() is also responsible for postprocessing 
	these vectors, removing NaN values to avoid training errors.
\end{Verbatim}

\noindent \texttt{\href{run:../model/model\_training.py}{model\_training.py}} is set up in such a way that it trains and saves models for each bird species in a loop, tracking the process using \texttt{tqdm}. If a custom species is desired, the \texttt{species\_list} variable must be modified to reflect it (ensure that the custom bird species has already undergone R preprocessing). These models are saved to \texttt{/model/output/models} via the \texttt{pickle} library.

\subsubsection{SDM Predicting}

SDM prediction takes place in \texttt{\href{run:../model/model\_prediction.py}{model\_prediciton.py}}. Its prototyping can again be found in \texttt{\href{run:../model/birdmig\_sdm.ipynb}{birdmig\_sdm.ipynb}}.
\\\\
Species distribution prediction uses the following process:
\begin{enumerate}
	\item Future climate rasters are loaded from \texttt{/model/input/future} and processed into an appropriate form factor
	\item Pickle files for each species is loaded from \texttt{/model/output/models}
	\item \texttt{impute()} from \texttt{pyimpute} is used to create distribution predictions in the form of TIFF images for each climate raster.
	\item TIFF images are converted to PNGs for use in the web application and stored in \texttt{/public/png-images}
\end{enumerate}

\noindent There exists a future climate raster for each year (2015-2100) for each SSP (ssp126, ssp245, ssp370, ssp585). These rasters are used to predict distribution for five species, making a total of 1,720 images.
\\\\
\texttt{\href{run:../model/model\_prediction.py}{model\_prediciton.py}} is comprised of a triple-nested for loop (species > ssp > year) and two functions, \texttt{make\_predictions()} and \texttt{create\_pngs()}. The former function is responsible for steps 1-3 listed above, while the latter function is responsible for step 4. TIFF images are converted to PNG through the use of \texttt{matplotlib}, and are saved in high resolution.

\section{Web Application}

\subsection{APIs}

\subsubsection{React}
Reactjs was used to created the frontend, and the main component is found under \texttt{/src/App.js}. In App.js, there are calls to the backend to gather data for the various components
located in the \texttt{/src/components} folder. These components include:
\begin{itemize}
	\item \texttt{BirdInfo.js}: returns the summary of the desired bird
	\item \texttt{ClimateChart.js}: returns temperature and precipitation graphs
	\item \texttt{HeatMap.js}: returns heatmap graph (see Leaflet section)
	\item \texttt{PolylineMap.js}: return bird trajectory graph (see Leaflet section)
	\item \texttt{SDMchart.js}: returns png of SDM output
	\item \texttt{PredictionControl.js}: contains code to display the year slider and SSP buttons that change the Climate and SDM Charts
\end{itemize}

\noindent In addition to these components, the sidebar and header are created in \texttt{App.js} which allows users to view different birds. All the styling is written up in \texttt{/src/App.css}. 


\subsubsection{Leaflet}
Leaflet was used to map bird migration patterns in two ways under the "Trajectory" component. 

The first map, "Individual Path", is created in \texttt{/src/components/PolylineMap.js}.
The function fetches data from the csv files under  \texttt{/backend/app/data}. The function first fetches all the tagged Bird IDs from the CSV for the desired bird. Then,
all the trajectories for the first Bird ID are fetched from the backend and drawn on OpenStreetMap using Leaflet.  Arrows are calculated between two points in \texttt{calculateBearings} 
function and added onto the map as well. 

The second map, "Aggregated Path", is created in \texttt{/src/component/HeatMap.js}. The function fetches all the latitude and longitude coordinates from the CSV of the desired bird in 
the \texttt{/backend/app/data} folder. Using Leaflet and the Leaflet plugin, \texttt{L.heatLayer}, a heat layer is generated showing the combined trajectory path of all the tagged birds on top of 
OpenStreetMap layer.  

\subsubsection{FastAPI}
The middleware and backend of the website was developed using python and FastAPI, in the file \texttt{base.py} located under \texttt{/backend/app}. The FastAPI backend primarily serves the purpose of quickly retrieving data and sending it to the front end as requested, allowing the front end user-interface to remain lightweight and easy to load quickly. FastAPI was used to build the backend due to its ease in creating end points for a frontend application to call, natively supporting features such as delayed requests and an intuitive way of passing arguments to the FastAPI backend.

The backend is defined predominantly by functions that handle various requests the front end makes in order to retrieve data to display. These functions and their functions are discussed in greater detail in the following section for RestFUL API.

\subsubsection{RestFUL}
RestFUL API was used to define how the end points were set up, partially due to built-in compatibility with FastAPI, but also due to its simple and intuitive interface. While RestFUL API primarily defines 4 primary methods for GET, PUT, POST, and DELETE, the final application was operational with a stateless backend, meaning only GET and PUT requests were used.
In the file base.py, GET and PUT requests were used to define the following end points:

\begin{itemize}
	\item \texttt{get\_temperature\_data()}: a GET endpoint that in turn performs a POST request to grid2.rcc-acis.org to obtain temperature data for the state of California throughout a year, which is specified per the user's request. Currently, this endpoint only utilizes the SSP8.5 scenario as other endpoints have not yet been developed by RCC-ACIS.
   	 \item \texttt{get\_precipitation\_data()}: a GET endpoint that also performs a POST request to grid2.rcc-acis.org to obtain precipitation data for the state of California throughout a year, specified per the user's request. Like the temperature endpoint, it also currently only utilizes the SSP8.5 scenario due to the limited development of other climate scenarios by RCC-ACIS.
	\item \texttt{get\_predictions()}: a PUT end point that retrieves the pre-computed predictions the Species Distribution Model makes about a specified bird, for a specified year based on specified emission data. While a GET request would have sufficed for its current utility, a PUT request was preferred as this could be expanded to implement a machine learning model to generate predictions dynamically should such a feature be planned in the future.
	\item \texttt{get\_trajectory\_data()}: a GET end point that retrieves a csv file containing the migration trajectory of a specific bird, identified by its unique bird ID, that can be used to generate individual trajectory maps of an individual bird. The trajectory information is discussed later in the subsection for Leaflet.
	\item \texttt{get\_bird\_ids()}: a GET end point that returns all unique bird IDs per a given species. This is used to by the front end component PolylineMap.js to search for a specified bird ID.
	\item \texttt{get\_heatmap\_data()}: a GET end point that retrieves heatmap data of a bird species, to display the aggregated paths of all birds catalogued for that species on the front end with HeatMap.js, as a heat map. This is discussed in further detail in the Leaflet subsection.
\end{itemize}

\subsection{Usage}
The website is designed to serve as an interactive tool for ornithologists, researchers, and bird watching enthusiasts. It allows users to:

\begin{itemize}
    \item Retrieve detailed information about various bird species, including their scientific names, general characteristics, and migration patterns.
    \item View and interact with climate data visualizations to understand how different climate variables such as temperature and precipitation affect bird migration.
    \item Predict future distributions of bird species using species distribution models (SDM) based on selected climate scenarios and years.
    \item Explore bird migration trajectories either as individual paths or aggregated heatmaps, providing a visual understanding of migration routes and densities.
\end{itemize}

Users can interact with various controls to select different birds, years, and emission scenarios, and the application dynamically updates the information and predictions displayed.

\subsection{Design}
The website is developed using a React frontend and a FastAPI backend, leveraging modern web technologies and frameworks to ensure a responsive and intuitive user experience. Key design features include:

\begin{itemize}
    \item \textbf{Modular Design:} The application is structured into several independent components like BirdInfo, SDMChart, and PolylineMap. Each component encapsulates a particular feature of the web app, facilitating seperation of concerns, readability, and scalability.
    \item \textbf{Data-Driven Interactions:} By integrating with external APIs and processing data through Python-based FastAPI, the application provides real-time data interactions and updates.
    \item \textbf{User-Centric Interfaces:} Emphasis on usability and accessibility, with interactive charts, maps, and dynamic controls that cater to both novice users and experienced researchers.
\end{itemize}

The application's architecture is designed to handle high volumes of data efficiently, utilizing caching and asynchronous data fetching to optimize performance and user experience.


\end{document}
